{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42210a1c-d840-4201-af23-830a91d06f71",
   "metadata": {},
   "source": [
    "# Week 2\n",
    "\n",
    "Linear sytems. Fundamentals of Numerical Computation, [Chapter 2](https://tobydriscoll.net/fnc-julia/linsys/overview.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcffea3-8d45-4fdb-901b-8b14221780e4",
   "metadata": {},
   "source": [
    "## Polynomial interpolation\n",
    "\n",
    "Given data $(t_i, y_i)$ for $i = 1, \\dots, n$ find a polynomial $p(t) = c_1 + c_2t + \\dots + c_nt^{n-1}$ that interpolates these data. That means that $p(t_i) = y_i$.\n",
    "\n",
    "Construct a linear system $Vc = y$ where V is a matrix composed of column vectors $V_j = t_i^j$, $c$ is the unknown coefficients, and $y$ is a column vector from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b9d88d-ce12-4639-ba8d-f7599a2c52a4",
   "metadata": {},
   "source": [
    "## Linear systems\n",
    "\n",
    "Note how to solve upper triangular systems $Ux = b$ by \"back substitution\", meaning starting by finding $x_4 = b_4/u_{44}$ and then proceeding to find $u_3$, etc.\n",
    "\n",
    "Similarly, solve a lower triangular system $Lx = b$ using \"forward substitution\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e9a9c3-3c44-4df7-a4dd-72a1b2fe61d4",
   "metadata": {},
   "source": [
    "## LU decomposition\n",
    "\n",
    "Outer ($uv^T$) and inner ($u^Tv$) products of column vectors.\n",
    "\n",
    "$L$ and $U$ are lower and upper triangular square matrices. The diagonal entries of $L$ are all 1. The diagonals of $U$ are non-zero. The matrices and their product are non-singular.\n",
    "\n",
    "Factor $A= LU$. Then solve $Ax = b$ using this decomposition.\n",
    "\n",
    "* Solve $LUx = b$ as follows\n",
    "  * Solve $Lz=b$ using forward substitution\n",
    "  * Solve $Ux = z$ using backward substition\n",
    "\n",
    "$L$ and $U$ systems are easier to solve than $A$ system. If you want to solve $Ax=b$ for one $A$ and many $b$ then you can save a lot of time using the LU decomposition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369dac00-a89f-4e94-9867-a924b40daa00",
   "metadata": {},
   "source": [
    "## Obtaining an LU decomposition\n",
    "\n",
    "Row-reduction is a linear transformation (swapping rows, scalar multiplication, adding a multiple of one row to another).\n",
    "\n",
    "* Row reduce A to U\n",
    "* Multiply elementary row operations together to get L (not completely obvious this is possible)\n",
    "* Pivot when necessary \n",
    "  * In case of zeros or near zero values when dividing; avoid subtractive cancellation. See Demo 2.6.1\n",
    "\n",
    "Herustic for permuting rows\n",
    "  * in step $i$ (zeroing elements of column $i$)\n",
    "    * start by putting the row with the largest magnitude entry in column $i$ into row $i$\n",
    "    * i.e., `j = argmax(abs.(A[:,i]))`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5579fa32-e042-4b5b-a9be-ae808fa23a39",
   "metadata": {},
   "source": [
    "## Stability\n",
    "\n",
    "A = [- epsilon, 1; 1 -1]. Demo 2.6.10\n",
    "\n",
    "If epsilon = 0, need to pivot. If epsilon >0, try without pivoting\n",
    "\n",
    "With epsilon = 1e-12 you lose about 10 digits of accuracy, but not due to the conditioning of the problem. Due to a poor algorithm. Pivoting gives the exact answer.\n",
    "\n",
    "No pivoting: L = [1, 0 ; -1/eps, 1 ] and U = [ -eps, 1, 0, 1/eps -1 ]\n",
    "\n",
    "L and U have condition number of about $1/\\epsilon^2$.\n",
    "\n",
    "There are A for which PLU is unstable, but these examples are generally unimportant and PLU algorithm is very useful (for solving general square linear systems)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d5ea05-ad36-4d90-bd79-5942174b3d8d",
   "metadata": {},
   "source": [
    "## Vector and Matrix norms (section 2.7)\n",
    "\n",
    "Function a vector space that measures the \"size\" or \"magnitude\" of a vector. Norm of 0 vector is 0 (and only this vector), norms are non-negative, scalar multiplication, triangle inequality.\n",
    "\n",
    "$$\\|x\\|_p = \\left(\\sum |x_i|^p\\right)^{1/p}$$\n",
    "\n",
    "$p = 1, 2, \\infty$ are common choices.\n",
    "\n",
    "unit vector $\\frac{x}{\\|x\\|}$.\n",
    "\n",
    "Frobenius norm for matrix $\\|A\\|_F = \\left(\\sum |A_{ij}|^2\\right)^{1/2}$.\n",
    "\n",
    "Lots more about Norms in the text and linear algebra courses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2e1ab5-0cb6-44e1-8bc4-88cc95b8329a",
   "metadata": {},
   "source": [
    "## Conditioning (Section 2.8)\n",
    "\n",
    "Start with $Ax=b$ and perturb it to $A(x+h) = b+d$.\n",
    "\n",
    "Find the relative change in the solution divided by the relative change in the 'data':\n",
    "\n",
    "$$ \\frac{\\frac{\\|h\\|}{\\|x\\|}}{\\frac{\\|d\\|}{\\|b\\|}}$$\n",
    "\n",
    "Bound on $h$: $Ax + Ah = b + d$, $Ah = d$, $h = A^{-1}d$, so $\\|h\\| \\leq \\|A^{-1}\\|\\cdot \\|d\\|$.\n",
    "\n",
    "Similar for $b$: $Ax = b$, $\\|A\\|\\cdot\\|x\\| \\geq \\|b\\|$.\n",
    "\n",
    "So \n",
    "\n",
    "$$\\frac{\\|h\\|\\cdot \\|b\\|}{\\|d\\|\\cdot\\|x\\|} \\leq \\|A\\|\\cdot\\|A^{-1}\\|$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f573ae28-ef73-40f9-9a97-816f49d3a317",
   "metadata": {},
   "source": [
    "The condition number of a matrix $A$ is $\\kappa(A) = \\|A\\|\\cdot\\|A^{-1}\\|$. The value depends on the norm (1,2, $\\infty$, etc.) but we don't worry too much about that for interpretation. If $A$ is singular then $\\kappa(A) = \\infty$. If $A$ is close to a singular matrix (a perturbation of entries similar to what is required to represent the entries as floating point) it will be effectively singular.\n",
    "\n",
    "Example: Hilbert matrix $A_{ij} = \\frac{1}{1+i+j}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edbe17a0-c204-4109-b018-d38745c2e137",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6708589868530223e8"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 6\n",
    "hilbert = [ 1/(1+i+j) for i in 1:N, j in 1:N]\n",
    "using LinearAlgebra\n",
    "cond(hilbert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df602b0-d2a8-4e9d-9461-b9ebb20fefd9",
   "metadata": {},
   "source": [
    "A symmetric matrix is positive definite if\n",
    "\n",
    "* $x^TAx > 0$ for all non-zero $x$, or equivalently\n",
    "* the eigenvalues of A are all strictly positive.\n",
    "\n",
    "If a matrix is [symmetric, positive definite](https://mathworld.wolfram.com/PositiveDefiniteMatrix.html) there is a Cholesky decomposition: $A = LDL^T = R^TR$ where $R$ is upper triangular with positive numbers on the diagonal.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3efb22-3c08-4eba-906c-d43d9a838837",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
